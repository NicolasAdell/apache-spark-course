# Taming Big Data with Apache Spark 4 and Python - Sundog Education

I finished this course, which covers the fundamental and advanced components of the Apache Spark ecosystem. Topics include:

- Installation and execution of Apache Spark in both local and cluster environments (Hadoop YARN and Amazon EMR).

- Use of RDDs, DataFrames, and Spark SQL for distributed data processing and analysis.

- Development of Structured Streaming workflows for real-time data pipelines.

- Formulation of large-scale analytical tasks as Spark problems.

- Implementation of distributed algorithms such as Breadth-First Search (BFS).

- Use of broadcast variables and accumulators for sharing state across cluster nodes.

- Application of MLLib for machine learning tasks including classification, regression, and recommendation.

- Use of GraphX for graph and network analysis.

- Performance tuning and troubleshooting of Spark jobs running in clustered environments.


[Course link](https://www.udemy.com/course/taming-big-data-with-apache-spark-hands-on/?couponCode=KEEPLEARNING)

